{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled29.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOG26SSuE2dTskEL1iHdQnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sra1panasa/Aitrflow_etcframeworks/blob/main/Airflow_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Airflow:\n",
        "\n",
        "* Apache Airflow is already a commonly used tool for scheduling data pipelines.\n",
        "\n",
        "* Apache Airflow is one significant scheduler for **programmatically scheduling, authoring, and monitoring the workflows in an organization**. It is mainly designed to orchestrate and handle complex pipelines of data.\n",
        "\n",
        "* Airflow can be described as a platform that helps define, monitoring and execute workflows. **In simple words, workflow is a sequence of steps that you take to accomplish a certain objective**\n",
        "\n",
        "* Also, Airflow is a code-first platform as well that is **designed with the notion that data pipelines can be best expressed as codes.**  \n",
        "\n",
        "* Airflow is a tool for automating and scheduling tasks and workflows. If you want to work efficiently as a data scientist, data analyst or data engineer it is essential to have a tool that can automate the processes you want to repeat on a regular basis. This can be anything from extracting, transforming and loading data for a regular analytics report to automatically re-training a machine learning mode\n",
        "\n",
        "* \n",
        "\n"
      ],
      "metadata": {
        "id": "__gBgEvCXWSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DAGS:**\n",
        "\n",
        "* At the heart of the tool is the concept of a DAG (Directed Acyclic Graph)\n",
        "\n",
        "* A DAG is a series of tasks that you want to run as part of your workflow. This might include something like extracting data via a SQL query, performing some calculations with Python and then loading the transformed data into a new table. **In Airflow each of these steps would be written as individual tasks in a DAG.**\n",
        "\n",
        "* A***irflow enables you to also specify the relationship between the tasks, any dependencies (e.g. data having loaded in a table before a task is run) and the order in which the tasks should be run.***\n",
        "\n",
        "* ***A DAG IS WRITTEN IN PYTHON and saved AS a .py file. The DAG_ID is used extensively by the tool to orchestrate the running of the DAGâ€™s.***"
      ],
      "metadata": {
        "id": "WTQEO5B1fMAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WS30Au_y0rUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q4NWvN0YkF8b"
      }
    }
  ]
}